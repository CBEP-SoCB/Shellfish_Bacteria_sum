---
title: "Shellfish Sanitation Program Data Assembly"
author: "Curtis C. Bohlen, Casco Bay Estuary Partnership."
date: "11/14/2020"
output:
  github_document:
    toc: true
    fig_width: 7
    fig_height: 5
---
<img
    src="https://www.cascobayestuary.org/wp-content/uploads/2014/04/logo_sm.jpg"
    style="position:absolute;top:10px;right:50px;" />

# Introduction
Initial loading of the raw observational data uncovered what appear to be data
format inconsistencies with the way the E. coli data were recorded. Here we
explore the data inconsistencies and make decisions regarding how to address
them.

# Install Libraries
```{r}
library(readr)
library(tidyverse)
```

# Load Data
## Folder References
```{r}
sibfldnm <- 'Original_Data'
parent <- dirname(getwd())
sibling <- file.path(parent,sibfldnm)
fn<- "Casco Bay WQ 15.19.csv"
```

## Read Data
Most of this code is to skip data columns that were used for data QA/QC, or to 
ensure dates and times are properly interpreted.

```{r}
path <- file.path(sibling, fn)
raw_data <- read_csv(path, 
    col_types = cols(
        ADVERSITY = col_skip(),  
        CATCH_COMMENTS = col_skip(), 
        CATCH_SEQ_NO = col_skip(),
        CATCH_UPDATE_DATE = col_skip(), 
        CATCH_UPDATE_USER = col_skip(),
        COL_METHOD = col_skip(), 
        DMR_CATCH_IDENTIFIER = col_skip(), 
        DMR_EFFORT_IDENTIFIER = col_skip(), 
        DMR_SAMPLE_IDENTIFIER = col_skip(), 
        EFFORT_COMMENTS = col_skip(),
        EFFORT_SEQ_NO = col_skip(), 
        EFFORT_START_DATE = col_date(format = "%Y-%m-%d"), 
        EFFORT_START_TIME = col_time(format = "%H:%M"), 
        EFFORT_UPDATE_DATE = col_skip(), 
        EFFORT_UPDATE_USER = col_skip(), 
        EXAM_DATE_TIME = col_skip(),
        INITIATED_BY = col_skip(), 
        INITIATED_DATE = col_skip(),
        LAB = col_skip(), 
        LAT_DD = col_skip(),
        LON_DD = col_skip(), 
        MISSED_STATION_CODE = col_skip(),
        ROW_NUMBER = col_skip(),
        SAMPLE_COMMENTS = col_skip(),
        SAMPLE_METHOD = col_skip(),
        SAMPLE_SEQ_NO = col_skip(), 
        SAMPLE_UPDATE_DATE = col_skip(), 
        SAMPLE_UPDATE_USER = col_skip(),
        STRATEGY = col_skip(),  
        TRIP_SEQ_NO = col_skip(),
        X = col_skip(),
        X1 = col_skip())) %>%
  mutate_at(c('LOCATION_ID', 'GROWING_AREA', 'OPEN_CLOSED_FLAG',
              'WIND_DIRECTION','TIDE_STAGE',
              'CURRENT_CLASSIFICATION_CODE',
              'CATEGORY'), factor) %>%
  mutate(YEAR = as.numeric(format(EFFORT_START_DATE, "%Y"))) %>%
  mutate(DOY = as.numeric(format(EFFORT_START_DATE, '%j'))) %>%
  mutate(MONTH = as.numeric(format(EFFORT_START_DATE, '%m')))
```

```{r}
with(raw_data, xtabs(~factor(format(EFFORT_START_DATE, '%m')) +factor(YEAR)))
```

Note that we the data sent to us does includes fewer November and no December
observations in 2019.  We may be better off restricting our analysis to
2015-2018.  Although we can check for seasonal patterns in the data as a check
on the necessity for that.

## Convert Orders of Factors
Since these are factors, and not ordered factors, this is merely to make it a
bit easier to interpret results. It also has the effect of changing the base
category for contrasts.  Note that this will not affect ordering of factors when
data is loaded in from CSV files, so this code (or something very like it) will
have to be run in any data analysis scripts.

## What levels are present?
```{r}
with(raw_data, levels(OPEN_CLOSED_FLAG))
with(raw_data, levels(WIND_DIRECTION))
with(raw_data, levels(TIDE_STAGE))
with(raw_data, levels(CURRENT_CLASSIFICATION_CODE))
with(raw_data, levels(CATEGORY))
```

## Reorder Some
```{r}
raw_data <- raw_data %>%
  mutate(WIND_DIRECTION =factor(WIND_DIRECTION,
                                levels = c("CL", "N", "NNE", "NE",
                                           "E","SE", "S","SW",
                                           "W", "NW"))) %>%
  mutate(CURRENT_CLASSIFICATION_CODE = factor(CURRENT_CLASSIFICATION_CODE,
                                              levels = c( 'A', 'CA', 'CR',
                                                          'R', 'P', 'X' ))) %>%
  mutate(TIDE_STAGE = factor(TIDE_STAGE, levels = c("L", "LF", "F", "HF",
                                                    "H", "HE", "E", "LE")))
```

## Simplify Column Names
Data names from DMR are long, in all caps, and awkward, so we want to simplify.
While we are at it, we want names to more or less match what we found in the p90
files.

```{r}
names(raw_data)
```

```{r}
raw_data <- raw_data %>%
  select(-FLOOD, -DELIVERY_TEMP_C) %>%
  rename(SDate = EFFORT_START_DATE,
         STime = EFFORT_START_TIME,
         SDateTime = START_DATE_TIME,
         Station =  LOCATION_ID,
         GROW_AREA =  GROWING_AREA,
         OpenClosed = OPEN_CLOSED_FLAG,
         WDIR = WIND_DIRECTION,
         Tide =  TIDE_STAGE,
         Class = CURRENT_CLASSIFICATION_CODE,
         Temp = TEMP_C,
         Sal = SALINITY_PCT,
         ColiScore = COL_SCORE,    # This value uses inappropriate substitution based on right censored values.
         RawColi = RAW_COL_SCORE)
```

```{r}
with(raw_data, sum(OpenClosed=='X', na.rm=TRUE))
with(raw_data, xtabs(~OpenClosed+factor(YEAR)))
```
So, the "X" flag was not used prior to 2018, and used infrequently in 2019.  We
need to understand what that flag means, but it may be appropriate for some
analyses to delete records with the X flag....

```{r}
with(raw_data, levels(WDIR))
with(raw_data, xtabs(~WDIR+factor(YEAR)))
```
Only "NW" was recorded in 2019. Most 2019 observations have no wind direction
code. NNE was only used in 2016. Otherwise, these data look inconsistent year
to year, so they may not be all that useful for analysis.

```{r}
with(raw_data, xtabs(~OpenClosed+CATEGORY))
```

So CATEGORY contains no useful information for us.

```{r}
with(raw_data, xtabs(~OpenClosed+Class))
```

Note that there's a (nearly) one to one match between category = Z, OPenClosed=X
and  Class= X.  Also note that Category = I was only used once.

We'd like to know what the meaning of the OpenClosed flag really is.  Was this a
flag showing condition before the sample, or a classification due to the results
of the sample.  We should be able to determine that based on the data itself.

## Remove Unreliable / Uninformative Categories
This code removes uninformative or inconsistent data and replaces "X" values
with NA.

```{r}
raw_data<-raw_data %>%
  select(-CATEGORY, -WDIR) %>%
  mutate(Class = factor(Class,levels = c( 'A', 'CA', 'CR',
                                          'R', 'P'))) %>%
  mutate(OpenClosed = factor(OpenClosed, levels = c('O', 'C')))
```

# Consistency of How Censored Data is Reported
```{r}
the_data <- raw_data %>%
  # Remove records with NAs for hte RawColi scores
  filter (! is.na(RawColi)) %>%
  #Identify censored data
  mutate(LCFlag = substr(RawColi,1,1)=='<') %>%
  mutate(RCFlag = substr(RawColi,1,1)=='>') %>%
  mutate(ColiVal = if_else(LCFlag,
                           substr(RawColi,2,nchar(RawColi)),
                           RawColi )) %>%
  mutate(ColiVal = if_else(RCFlag,
                           substr(RawColi,2,nchar(RawColi)),
                           ColiVal)) %>%
  mutate(ColiVal = as.numeric(ColiVal))
 
```

```{r}
ggplot(the_data, aes(ColiVal, ColiScore)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0)
```

So, here we see a couple of things:

1. DMR replaced "raw" observations with arbitrary larger or smaller values when
   they produced the `ColiScore` values.  We may be able to do better, or
   handle these censored data with more sophistication.
   
2. There's a strange observation or group of observations that was recorded as
   < something that is well above the nominal detection limits.

## Left Censoring
```{r}
ggplot(the_data[the_data$ColiScore<5,], aes(ColiVal, ColiScore)) +
  geom_point(alpha = 0.1) +
  geom_abline(slope = 1, intercept = 0)
```
So we have some inconsistency of how censored observations at the low end were
handled. DMR USUALLY recorded their `COL_SCORE` as 1.9, presumably because that
is "slightly below 2.0", which is the true reporting limit for  the method. But
a few times, DMR also reports the 'RAW_COL_SCORE' as "1.9", instead of "<2."

```{r}
the_data %>%
filter(ColiScore<2) %>%
  filter(RawColi != '<2') %>%
  select(SDate, ColiScore, RawColi, ColiVal, LCFlag)
```

Those anomalous entries come almost entirely from one day.  Our guess is that
the detection limit for the method used does not vary, and so, these
observations were incorrectly recorded. Luckily, the apparent inconsistencies
are very rare, and can be adjusted with little concern for loss of accuracy.

```{r}
the_data$ColiVal[the_data$ColiVal==1.9 ] = 2.0
```

What about other inconsistencies, with regard to what values are flagged as left
censored?
```{r}
the_data %>%
  filter(LCFlag) %>%
  group_by(ColiVal) %>%
  summarise(L = first(ColiVal)) %>%
  pull(L) 
```

```{r}
the_data[the_data$ColiVal==18 & the_data$LCFlag, ] %>%
  select(SDate, ColiScore, RawColi, ColiVal, LCFlag)
```

DMR recorded three samples as left censored, with a reporting limit of 18. The
observations were all from a single day.  This elevated detection
limit is problematic, as it cold bias any unsophisticated handling of
non-detects.

We could consider removing these observations as anomalies, but
there is no internal justification for doing so.  For now, we leave them in 
place.

# Right Censoring
```{r}
ggplot(the_data[the_data$ColiScore>1000,], aes(ColiVal, ColiScore)) +
  geom_point(alpha = 0.1) +
  geom_abline(slope = 1, intercept = 0)
```

```{r}
the_data %>%
filter(ColiScore>1500) %>%
  select(ColiScore, RawColi, ColiVal, RCFlag)
```
So, DMR used an arbitrary value of 1700 as being "slightly" higher than the 1600
upper detection limit.
